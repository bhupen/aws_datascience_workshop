{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation with a Amazon SageMaker Processing Job and SparkML\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge.\n",
    "\n",
    "Often, distributed data processing frameworks such as Spark are used to pre-process data sets in order to prepare them for training. In this notebook we'll use Amazon SageMaker Processing, and leverage the power of Spark in a managed SageMaker environment to run our processing workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/prepare_dataset.png)\n",
    "\n",
    "![](img/processing.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Setup\n",
    "1. Use Amazon SageMaker Processing Job to execute a Spark ML Transformation\n",
    "1. Setup Input Data\n",
    "1. Setup Output Data\n",
    "1. Build a Spark container for running the processing job\n",
    "1. Run the Processing Job using Amazon SageMaker\n",
    "1. Inspect the Processed Output Dta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Add the following policies to your SageMaker Execution Role:  \n",
    "* `EC2ContainerRegistry`\n",
    "* Permissions: `List`, `Read`, `Write` \n",
    "* Repository:  `amazon-reviews-spark-processor`\n",
    "\n",
    "Let's start by specifying:\n",
    "* The S3 bucket and prefixes that you use for training and model data. Use the default bucket specified by the Amazon SageMaker session.\n",
    "* The IAM role ARN used to give processing and training access to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Amazon SageMaker Processing Job to Run a Spark ML Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of previous Scikit Processing Job to use to construct the input for our Spark ML Processing Job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Spark Docker Image to Run the Processing Job\n",
    "\n",
    "An example Spark container is included in the `./container` directory of this example. The container handles the bootstrapping of all Spark configuration, and serves as a wrapper around the `spark-submit` CLI. At a high level the container provides:\n",
    "* A set of default Spark/YARN/Hadoop configurations\n",
    "* A bootstrapping script for configuring and starting up Spark master/worker nodes\n",
    "* A wrapper around the `spark-submit` CLI to submit a Spark application\n",
    "\n",
    "\n",
    "After the container build and push process is complete, use the Amazon SageMaker Python SDK to submit a managed, distributed Spark application that performs our dataset processing.\n",
    "\n",
    "Build the example Spark container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = 'amazon-reviews-spark-processor'\n",
    "docker_tag = 'latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  3.023MB\n",
      "Step 1/33 : FROM openjdk:8-jre-slim\n",
      " ---> bf20b099be53\n",
      "Step 2/33 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 568bcfe760c7\n",
      "Step 3/33 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> 5e8f2be78f61\n",
      "Step 4/33 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> f35677fba605\n",
      "Step 5/33 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> c3d1f7a6fe79\n",
      "Step 6/33 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 251601397074\n",
      "Step 7/33 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> e409300c31d9\n",
      "Step 8/33 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> 9124e9a57eb0\n",
      "Step 9/33 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> c7d3e56eeae5\n",
      "Step 10/33 : ENV HADOOP_VERSION 3.0.0\n",
      " ---> Using cache\n",
      " ---> 2f920737f94b\n",
      "Step 11/33 : ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\n",
      " ---> Using cache\n",
      " ---> 06620a5da59c\n",
      "Step 12/33 : ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
      " ---> Using cache\n",
      " ---> 21124cee717c\n",
      "Step 13/33 : ENV PATH $PATH:$HADOOP_HOME/bin\n",
      " ---> Using cache\n",
      " ---> e7052c2c29c7\n",
      "Step 14/33 : RUN curl -sL --retry 3   \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\"   | gunzip   | tar -x -C /usr/  && rm -rf $HADOOP_HOME/share/doc  && chown -R root:root $HADOOP_HOME\n",
      " ---> Using cache\n",
      " ---> e18cc6894089\n",
      "Step 15/33 : ENV SPARK_VERSION 2.4.5\n",
      " ---> Using cache\n",
      " ---> d3a5d307d17b\n",
      "Step 16/33 : ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop\n",
      " ---> Using cache\n",
      " ---> 5d1fc5374002\n",
      "Step 17/33 : ENV SPARK_HOME /usr/spark-${SPARK_VERSION}\n",
      " ---> Using cache\n",
      " ---> 797d424a25e4\n",
      "Step 18/33 : ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\n",
      " ---> Using cache\n",
      " ---> d7afba1e1003\n",
      "Step 19/33 : ENV PATH $PATH:${SPARK_HOME}/bin\n",
      " ---> Using cache\n",
      " ---> f02633ec1d60\n",
      "Step 20/33 : RUN curl -sL --retry 3   \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz\"   | gunzip   | tar x -C /usr/  && mv /usr/$SPARK_PACKAGE $SPARK_HOME  && chown -R root:root $SPARK_HOME\n",
      " ---> Using cache\n",
      " ---> 482afdbb764b\n",
      "Step 21/33 : ENV PYSPARK_PYTHON=/usr/bin/python3\n",
      " ---> Using cache\n",
      " ---> 64685bc65925\n",
      "Step 22/33 : ENV PATH=\"/usr/bin:/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> af16cc999111\n",
      "Step 23/33 : ENV YARN_RESOURCEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 44ca47cfa7ab\n",
      "Step 24/33 : ENV YARN_NODEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> a40533ef0f69\n",
      "Step 25/33 : ENV HDFS_NAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 5c03a4669ceb\n",
      "Step 26/33 : ENV HDFS_DATANODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> a0372699ca6d\n",
      "Step 27/33 : ENV HDFS_SECONDARYNAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 2c1731b21ce2\n",
      "Step 28/33 : COPY program /opt/program\n",
      " ---> Using cache\n",
      " ---> 3465fb728b4e\n",
      "Step 29/33 : RUN chmod +x /opt/program/submit\n",
      " ---> Using cache\n",
      " ---> 34456ed600c3\n",
      "Step 30/33 : COPY hadoop-config /opt/hadoop-config\n",
      " ---> Using cache\n",
      " ---> 1c0b777da037\n",
      "Step 31/33 : COPY jars /usr/jars\n",
      " ---> a55d9e64a0de\n",
      "Step 32/33 : WORKDIR $SPARK_HOME\n",
      " ---> Running in b22a5a99baea\n",
      "Removing intermediate container b22a5a99baea\n",
      " ---> 80359924ee7f\n",
      "Step 33/33 : ENTRYPOINT [\"/opt/program/submit\"]\n",
      " ---> Running in e701ca995827\n",
      "Removing intermediate container e701ca995827\n",
      " ---> 5e16db1c9125\n",
      "Successfully built 5e16db1c9125\n",
      "Successfully tagged amazon-reviews-spark-processor:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $docker_repo:$docker_tag -f container/Dockerfile ./container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Amazon Elastic Container Registry (Amazon ECR) repository for the Spark container and push the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "086401037028.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "image_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account_id, region, docker_repo, docker_tag)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ECR repository and push docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryNotFoundException) when calling the DescribeRepositories operation: The repository with name 'amazon-reviews-spark-processor' does not exist in the registry with id '086401037028'\n",
      "{\n",
      "    \"repository\": {\n",
      "        \"repositoryArn\": \"arn:aws:ecr:us-west-2:086401037028:repository/amazon-reviews-spark-processor\",\n",
      "        \"registryId\": \"086401037028\",\n",
      "        \"repositoryName\": \"amazon-reviews-spark-processor\",\n",
      "        \"repositoryUri\": \"086401037028.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor\",\n",
      "        \"createdAt\": 1587838682.0,\n",
      "        \"imageTagMutability\": \"MUTABLE\",\n",
      "        \"imageScanningConfiguration\": {\n",
      "            \"scanOnPush\": false\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws ecr describe-repositories --repository-names $docker_repo || aws ecr create-repository --repository-name $docker_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag $docker_repo:$docker_tag $image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [086401037028.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor]\n",
      "\n",
      "\u001b[1B64ea2c17: Preparing \n",
      "\u001b[1B0ee2b131: Preparing \n",
      "\u001b[1Bba328002: Preparing \n",
      "\u001b[1B1eb866e9: Preparing \n",
      "\u001b[1B6f8023d3: Preparing \n",
      "\u001b[1Bfa33a696: Preparing \n",
      "\u001b[1B6b9d6e4d: Preparing \n",
      "\u001b[1B7cc5f91d: Preparing \n",
      "\u001b[1Beb2ae899: Preparing \n",
      "\u001b[1B3a42c7f7: Preparing \n",
      "\u001b[1Ba9b45c60: Preparing \n",
      "\u001b[1B3b6fbb91: Preparing \n",
      "\u001b[1Bac5e9bd9: Preparing \n",
      "\u001b[1B0d58a380: Preparing \n",
      "\u001b[6B3a42c7f7: Pushed   490.3MB/481.8MB\u001b[13A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[7A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[2A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[KPushing  245.7MB/481.8MB\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[Klatest: digest: sha256:121bfffc5888bf5f38ee5d331e5aada2bad3a12231cc30ca62c6ab0d9c3f87be size: 3472\n"
     ]
    }
   ],
   "source": [
    "!docker push $image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Job using Amazon SageMaker Processing Jobs\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the Spark container that was just built, and a SparkML script for processing in the job configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the Spark processing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "from __future__ import unicode_literals\r\n",
      "\r\n",
      "import time\r\n",
      "import sys\r\n",
      "import os\r\n",
      "import shutil\r\n",
      "import csv\r\n",
      "\r\n",
      "import pyspark\r\n",
      "from pyspark.sql import SparkSession\r\n",
      "from pyspark.ml import Pipeline\r\n",
      "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType\r\n",
      "from pyspark.sql.functions import *\r\n",
      "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\r\n",
      "from pyspark.ml.linalg import DenseVector\r\n",
      "from pyspark.sql.functions import split\r\n",
      "from pyspark.sql.functions import udf, col\r\n",
      "from pyspark.sql.types import ArrayType, DoubleType\r\n",
      "from pyspark.ml.feature import PCA, StandardScaler\r\n",
      "\r\n",
      "\r\n",
      "def to_array(col):\r\n",
      "    def to_array_internal(v):\r\n",
      "        if v:\r\n",
      "            return v.toArray().tolist()\r\n",
      "        else:\r\n",
      "            print('EmptyV: {}'.format(v))\r\n",
      "            return []\r\n",
      "    return udf(to_array_internal, ArrayType(DoubleType())).asNondeterministic()(col)\r\n",
      "\r\n",
      "\r\n",
      "def transform(spark, s3_input_data, s3_output_train_data, s3_output_validation_data, s3_output_test_data): \r\n",
      "    print('Processing {} => {}'.format(s3_input_data, s3_output_train_data, s3_output_validation_data, s3_output_test_data))\r\n",
      " \r\n",
      "    schema = StructType([\r\n",
      "        StructField('marketplace', StringType(), True),\r\n",
      "        StructField('customer_id', StringType(), True),\r\n",
      "        StructField('review_id', StringType(), True),\r\n",
      "        StructField('product_id', StringType(), True),\r\n",
      "        StructField('product_parent', StringType(), True),\r\n",
      "        StructField('product_title', StringType(), True),\r\n",
      "        StructField('product_category', StringType(), True),\r\n",
      "        StructField('star_rating', IntegerType(), True),\r\n",
      "        StructField('helpful_votes', IntegerType(), True),\r\n",
      "        StructField('total_votes', IntegerType(), True),\r\n",
      "        StructField('vine', StringType(), True),\r\n",
      "        StructField('verified_purchase', StringType(), True),\r\n",
      "        StructField('review_headline', StringType(), True),\r\n",
      "        StructField('review_body', StringType(), True),\r\n",
      "        StructField('review_date', StringType(), True)\r\n",
      "    ])\r\n",
      "    \r\n",
      "    df_csv = spark.read.csv(path=s3_input_data,\r\n",
      "                            sep='\\t',\r\n",
      "                            schema=schema,\r\n",
      "                            header=True,\r\n",
      "                            quote=None)\r\n",
      "    df_csv.show()\r\n",
      "\r\n",
      "    # This dataset should already be clean, but always good to double-check\r\n",
      "    print('Showing null review_body rows...')\r\n",
      "    df_csv.where(col('review_body').isNull()).show()\r\n",
      "\r\n",
      "    df_csv_cleaned = df_csv.na.drop(subset=['review_body'])\r\n",
      "    df_csv_cleaned.where(col('review_body').isNull()).show()\r\n",
      "   \r\n",
      "    tokenizer = Tokenizer(inputCol='review_body', outputCol='words')\r\n",
      "    wordsData = tokenizer.transform(df_csv_cleaned)\r\n",
      "    \r\n",
      "    hashingTF = HashingTF(inputCol='words', outputCol='raw_features', numFeatures=1000)\r\n",
      "    featurizedData = hashingTF.transform(wordsData)\r\n",
      "    \r\n",
      "    # While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\r\n",
      "    # 1) compute the IDF vector \r\n",
      "    # 2) scale the term frequencies by IDF\r\n",
      "    # Therefore, we cache the result of the HashingTF transformation above to speed up the 2nd pass\r\n",
      "    featurizedData.cache()\r\n",
      "\r\n",
      "    # spark.mllib's IDF implementation provides an option for ignoring terms\r\n",
      "    # which occur in less than a minimum number of documents.\r\n",
      "    # In such cases, the IDF for these terms is set to 0.\r\n",
      "    # This feature can be used by passing the minDocFreq value to the IDF constructor.\r\n",
      "    idf = IDF(inputCol='raw_features', outputCol='features') #, minDocFreq=2)\r\n",
      "    idfModel = idf.fit(featurizedData)\r\n",
      "    features_df = idfModel.transform(featurizedData)\r\n",
      "    features_df.select('star_rating', 'features').show()\r\n",
      "\r\n",
      "    num_features=300\r\n",
      "    pca = PCA(k=num_features, inputCol='features', outputCol='pca_features')\r\n",
      "    pca_model = pca.fit(features_df)\r\n",
      "    pca_features_df = pca_model.transform(features_df).select('star_rating', 'pca_features')\r\n",
      "    pca_features_df.show(truncate=False)\r\n",
      "\r\n",
      "    standard_scaler = StandardScaler(inputCol='pca_features', outputCol='scaled_pca_features')\r\n",
      "    standard_scaler_model = standard_scaler.fit(pca_features_df)\r\n",
      "    standard_scaler_features_df = standard_scaler_model.transform(pca_features_df).select('star_rating', 'scaled_pca_features')\r\n",
      "    standard_scaler_features_df.show(truncate=False)\r\n",
      "\r\n",
      "    expanded_features_df = (standard_scaler_features_df.withColumn('f', to_array(col('scaled_pca_features')))\r\n",
      "        .select(['star_rating'] + [col('f')[i] for i in range(num_features)]))\r\n",
      "    expanded_features_df.show()\r\n",
      "\r\n",
      "    train_df, validation_df, test_df = expanded_features_df.randomSplit([0.9, 0.05, 0.05])\r\n",
      "\r\n",
      "    train_df.write.csv(path=s3_output_train_data,\r\n",
      "                       header=None,\r\n",
      "                       quote=None) #,\r\n",
      "    print('Wrote to output file:  {}'.format(s3_output_train_data))\r\n",
      "\r\n",
      "    validation_df.write.csv(path=s3_output_validation_data,\r\n",
      "                            header=None,\r\n",
      "                            quote=None) #,\r\n",
      "    print('Wrote to output file:  {}'.format(s3_output_validation_data))\r\n",
      "\r\n",
      "    test_df.write.csv(path=s3_output_test_data,\r\n",
      "                       header=None,\r\n",
      "                       quote=None) \r\n",
      "    print('Wrote to output file:  {}'.format(s3_output_test_data))\r\n",
      "\r\n",
      "\r\n",
      "def main():\r\n",
      "    spark = SparkSession.builder.appName('AmazonReviewsSparkProcessor').getOrCreate()\r\n",
      "\r\n",
      "    # Convert command line args into a map of args\r\n",
      "    args_iter = iter(sys.argv[1:])\r\n",
      "    args = dict(zip(args_iter, args_iter))\r\n",
      "\r\n",
      "    # Retrieve the args and replace 's3://' with 's3a://' (used by Spark)\r\n",
      "    s3_input_data = args['s3_input_data'].replace('s3://', 's3a://')\r\n",
      "    print(s3_input_data)\r\n",
      "\r\n",
      "    s3_output_train_data = args['s3_output_train_data'].replace('s3://', 's3a://')\r\n",
      "    print(s3_output_train_data)\r\n",
      "\r\n",
      "    s3_output_validation_data = args['s3_output_validation_data'].replace('s3://', 's3a://')\r\n",
      "    print(s3_output_validation_data)\r\n",
      "\r\n",
      "    s3_output_test_data = args['s3_output_test_data'].replace('s3://', 's3a://')\r\n",
      "    print(s3_output_test_data)\r\n",
      "\r\n",
      "    transform(spark, s3_input_data, s3_output_train_data, s3_output_validation_data, s3_output_test_data)\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "cat preprocess-spark-text-to-tfidf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "processor = ScriptProcessor(base_job_name='spark-amazon-reviews-processor',\n",
    "                            image_uri=image_uri,\n",
    "                            command=['/opt/program/submit'],\n",
    "                            role=role,\n",
    "                            instance_count=2, # instance_count needs to be > 1 or you will see the following error:  \"INFO yarn.Client: Application report for application_ (state: ACCEPTED)\"\n",
    "                            instance_type='ml.r5.8xlarge',\n",
    "                            env={'mode': 'python'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the Spark Processing Job\n",
    "\n",
    "_Notes on Invoking from Lambda:_\n",
    "* However, if we use the boto3 SDK (ie. with a Lambda), we need to copy the `preprocess-text-to-tfidf.py` file to S3 and specify the everything include --py-files, etc.\n",
    "* We would need to do the following before invoking the Lambda:\n",
    "     !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/code/preprocess.py\n",
    "     !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/py_files/preprocess.py\n",
    "* Then reference the s3://<location> above in the --py-files, etc.\n",
    "* See Lambda example code in this same project for more details.\n",
    "\n",
    "_Notes on not using ProcessingInput and Output:_\n",
    "* Since Spark natively reads/writes from/to S3 using s3a://, we can avoid the copy required by ProcessingInput and ProcessingOutput (FullyReplicated or ShardedByS3Key) and just specify the S3 input and output buckets/prefixes._\"\n",
    "* See https://github.com/awslabs/amazon-sagemaker-examples/issues/994 for issues related to using /opt/ml/processing/input/ and output/\n",
    "* If we use ProcessingInput, the data will be copied to each node (which we don't want in this case since Spark already handles this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-086401037028/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "s3_input_data = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-25 16:31:58   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2020-04-25 16:32:07   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "output_prefix = 'amazon-reviews-spark-processor-{}'.format(timestamp_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-086401037028/amazon-reviews-spark-processor-2020-04-25-18-18-29/output/tfidf-train\n",
      "s3://sagemaker-us-west-2-086401037028/amazon-reviews-spark-processor-2020-04-25-18-18-29/output/tfidf-validation\n",
      "s3://sagemaker-us-west-2-086401037028/amazon-reviews-spark-processor-2020-04-25-18-18-29/output/tfidf-test\n"
     ]
    }
   ],
   "source": [
    "train_data_tfidf_output = 's3://{}/{}/output/tfidf-train'.format(bucket, output_prefix)\n",
    "validation_data_tfidf_output = 's3://{}/{}/output/tfidf-validation'.format(bucket, output_prefix)\n",
    "test_data_tfidf_output = 's3://{}/{}/output/tfidf-test'.format(bucket, output_prefix)\n",
    "\n",
    "print(train_data_tfidf_output)\n",
    "print(validation_data_tfidf_output)\n",
    "print(test_data_tfidf_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-amazon-reviews-processor-2020-04-25-18-18-29-763\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-086401037028/spark-amazon-reviews-processor-2020-04-25-18-18-29-763/input/code/preprocess-spark-text-to-tfidf.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'dummy-output', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-086401037028/spark-amazon-reviews-processor-2020-04-25-18-18-29-763/output/dummy-output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "processor.run(code='preprocess-spark-text-to-tfidf.py',\n",
    "              arguments=['s3_input_data', s3_input_data,\n",
    "                         's3_output_train_data', train_data_tfidf_output,\n",
    "                         's3_output_validation_data', validation_data_tfidf_output,\n",
    "                         's3_output_test_data', test_data_tfidf_output,                         \n",
    "              ],\n",
    "              # We need this dummy output to allow us to call \n",
    "              #    ProcessingJob.from_processing_name() later \n",
    "              #    to describe the job and poll for Completed status\n",
    "              outputs=[\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='dummy-output',\n",
    "                                        source='/opt/ml/processing/output')\n",
    "              ],          \n",
    "              logs=True,\n",
    "              wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=spark-amazon-reviews-processor-2020-04-25-18-18-29-763;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "spark_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, spark_processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-086401037028/amazon-reviews-spark-processor-2020-04-25-18-18-29/?region=us-west-2&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# This is different than the job name because we are not using ProcessingOutput's in this Spark ML case.\n",
    "spark_processing_job_s3_output_prefix = output_prefix\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, spark_processing_job_s3_output_prefix, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Wait Until the Processing Job Completes\n",
    "Re-run this next cell until the job status shows `Completed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-086401037028/spark-amazon-reviews-processor-2020-04-25-18-18-29-763/input/code/preprocess-spark-text-to-tfidf.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'dummy-output', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-086401037028/spark-amazon-reviews-processor-2020-04-25-18-18-29-763/output/dummy-output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'spark-amazon-reviews-processor-2020-04-25-18-18-29-763', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.r5.8xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '086401037028.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor:latest', 'ContainerEntrypoint': ['/opt/program/submit', '/opt/ml/processing/input/code/preprocess-spark-text-to-tfidf.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-west-2-086401037028/amazon-reviews-pds/tsv/', 's3_output_train_data', 's3://sagemaker-us-west-2-086401037028/amazon-reviews-spark-processor-2020-04-25-18-18-29/output/tfidf-train', 's3_output_validation_data', 's3://sagemaker-us-west-2-086401037028/amazon-reviews-spark-processor-2020-04-25-18-18-29/output/tfidf-validation', 's3_output_test_data', 's3://sagemaker-us-west-2-086401037028/amazon-reviews-spark-processor-2020-04-25-18-18-29/output/tfidf-test']}, 'Environment': {'mode': 'python'}, 'RoleArn': 'arn:aws:iam::086401037028:role/TeamRole', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:086401037028:processing-job/spark-amazon-reviews-processor-2020-04-25-18-18-29-763', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 4, 25, 18, 18, 30, 138000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 4, 25, 18, 18, 30, 138000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '2f92c37a-26a1-4d29-9bf0-08754541840a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '2f92c37a-26a1-4d29-9bf0-08754541840a', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1929', 'date': 'Sat, 25 Apr 2020 18:18:29 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=spark_processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output Dataset\n",
    "\n",
    "\n",
    "## The next cells will not work properly until the job completes above.\n",
    "\n",
    "\n",
    "Take a look at a few rows of the transformed dataset to make sure the processing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $train_data_tfidf_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $validation_data_tfidf_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $test_data_tfidf_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
