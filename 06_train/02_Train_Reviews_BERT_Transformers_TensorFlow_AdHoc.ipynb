{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade pip\n",
    "!pip install -q wrapt --upgrade --ignore-installed\n",
    "!pip install -q tensorflow==2.1.0\n",
    "!pip install -q transformers==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import argparse\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from transformers import TextClassificationPipeline\n",
    "from transformers.configuration_distilbert import DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH=128\n",
    "BATCH_SIZE=16\n",
    "EPOCHS=1\n",
    "STEPS_PER_EPOCH=250\n",
    "VALIDATION_STEPS=50\n",
    "TEST_STEPS=50\n",
    "CLASSES = [1, 2, 3, 4, 5]\n",
    "NUM_GPUS=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data_and_label_from_record(record):\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids']\n",
    "    }\n",
    "    y = record['label_ids']\n",
    "\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_based_input_dataset_builder(channel,\n",
    "                                     input_filenames,\n",
    "                                     pipe_mode,\n",
    "                                     is_training,\n",
    "                                     drop_remainder):\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "\n",
    "    if pipe_mode:\n",
    "        print('***** Using pipe_mode with channel {}'.format(channel))\n",
    "        from sagemaker_tensorflow import PipeModeDataset\n",
    "        dataset = PipeModeDataset(channel=channel,\n",
    "                                  record_format='TFRecord')\n",
    "    else:\n",
    "        print('***** Using input_filenames {}'.format(input_filenames))\n",
    "        dataset = tf.data.TFRecordDataset(input_filenames)\n",
    "\n",
    "    dataset = dataset.repeat(EPOCHS * STEPS_PER_EPOCH)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    name_to_features = {\n",
    "      \"input_ids\": tf.io.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n",
    "      \"input_mask\": tf.io.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n",
    "      \"segment_ids\": tf.io.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n",
    "      \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    def _decode_record(record, name_to_features):\n",
    "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "        return tf.io.parse_single_example(record, name_to_features)\n",
    "        \n",
    "    dataset = dataset.apply(\n",
    "        tf.data.experimental.map_and_batch(\n",
    "          lambda record: _decode_record(record, name_to_features),\n",
    "          batch_size=BATCH_SIZE,\n",
    "          drop_remainder=drop_remainder,\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "    dataset.cache()\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(seed=42,\n",
    "                                  buffer_size=10,\n",
    "                                  reshuffle_each_iteration=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = './data-tfrecord/bert-train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_filenames ['./data-tfrecord/bert-train/part-algo-2-amazon_reviews_us_Digital_Software_v1_00.tfrecord', './data-tfrecord/bert-train/part-algo-4-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord']\n",
      "***** Using input_filenames ['./data-tfrecord/bert-train/part-algo-2-amazon_reviews_us_Digital_Software_v1_00.tfrecord', './data-tfrecord/bert-train/part-algo-4-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord']\n",
      "WARNING:tensorflow:From <ipython-input-5-2c7a824e813d>:38: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    }
   ],
   "source": [
    "train_data_filenames = glob('{}/*.tfrecord'.format(train_data))\n",
    "print('train_data_filenames {}'.format(train_data_filenames))\n",
    "train_dataset = file_based_input_dataset_builder(\n",
    "    channel='train',\n",
    "    input_filenames=train_data_filenames,\n",
    "    pipe_mode=False,\n",
    "    is_training=True,\n",
    "    drop_remainder=False).map(select_data_and_label_from_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = './data-tfrecord/bert-validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_data_filenames ['./data-tfrecord/bert-validation/part-algo-2-amazon_reviews_us_Digital_Software_v1_00.tfrecord', './data-tfrecord/bert-validation/part-algo-4-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord']\n",
      "***** Using input_filenames ['./data-tfrecord/bert-validation/part-algo-2-amazon_reviews_us_Digital_Software_v1_00.tfrecord', './data-tfrecord/bert-validation/part-algo-4-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "validation_data_filenames = glob('{}/*.tfrecord'.format(validation_data))\n",
    "\n",
    "print('validation_data_filenames {}'.format(validation_data_filenames))\n",
    "validation_dataset = file_based_input_dataset_builder(\n",
    "    channel='validation',\n",
    "    input_filenames=validation_data_filenames,\n",
    "    pipe_mode=False,\n",
    "    is_training=False,\n",
    "    drop_remainder=False).map(select_data_and_label_from_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = './data-tfrecord/bert-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data-tfrecord/bert-test/part-algo-2-amazon_reviews_us_Digital_Software_v1_00.tfrecord', './data-tfrecord/bert-test/part-algo-4-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord']\n",
      "***** Using input_filenames ['./data-tfrecord/bert-test/part-algo-2-amazon_reviews_us_Digital_Software_v1_00.tfrecord', './data-tfrecord/bert-test/part-algo-4-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "test_data_filenames = glob('{}/*.tfrecord'.format(test_data))\n",
    "\n",
    "print(test_data_filenames)\n",
    "\n",
    "test_dataset = file_based_input_dataset_builder(\n",
    "    channel='test',\n",
    "    input_filenames=test_data_filenames,\n",
    "    pipe_mode=False,\n",
    "    is_training=False,\n",
    "    drop_remainder=False).map(select_data_and_label_from_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee5dd97c4be4b679de2e2e3c8430b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10089894c2634235add32a8bd7ef069c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = DistilBertConfig.from_pretrained('distilbert-base-uncased',\n",
    "                                          num_labels=len(CLASSES))\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', \n",
    "                                                              config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the fine-tuning here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  3845      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,957,317\n",
      "Trainable params: 66,957,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "model.layers[0].trainable = True\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "\n",
    "log_dir = './tensorboard/'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "callbacks.append(tensorboard_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 250 steps, validate for 50 steps\n",
      "250/250 [==============================] - 416s 2s/step - loss: 1.1174 - accuracy: 0.5820 - val_loss: 0.8488 - val_accuracy: 0.6925\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    shuffle=True,\n",
    "                    epochs=EPOCHS,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                    validation_data=validation_dataset,\n",
    "                    validation_steps=VALIDATION_STEPS,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model <transformers.modeling_tf_distilbert.TFDistilBertForSequenceClassification object at 0x7f5cf56f7d68>\n"
     ]
    }
   ],
   "source": [
    "print('Trained model {}'.format(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Holdout Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 21s 410ms/step - loss: 0.8606 - accuracy: 0.6775\n",
      "[0.8606372255086899, 0.6775]\n"
     ]
    }
   ],
   "source": [
    "test_history = model.evaluate(test_dataset,\n",
    "                              steps=TEST_STEPS,\n",
    "                              callbacks=callbacks)\n",
    "print(test_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './fine-tuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $model_dir\n",
    "model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 261692\r\n",
      "drwxrwxr-x 2 ec2-user ec2-user      4096 Apr 25 18:33 .\r\n",
      "drwxrwxr-x 9 ec2-user ec2-user      4096 Apr 25 18:33 ..\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user      1358 Apr 25 18:33 config.json\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 267959068 Apr 25 18:33 tf_model.h5\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al $model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"_num_labels\": 5,\r\n",
      "  \"activation\": \"gelu\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"DistilBertForMaskedLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.1,\r\n",
      "  \"bad_words_ids\": null,\r\n",
      "  \"bos_token_id\": null,\r\n",
      "  \"decoder_start_token_id\": null,\r\n",
      "  \"dim\": 768,\r\n",
      "  \"do_sample\": false,\r\n",
      "  \"dropout\": 0.1,\r\n",
      "  \"early_stopping\": false,\r\n",
      "  \"eos_token_id\": null,\r\n",
      "  \"finetuning_task\": null,\r\n",
      "  \"hidden_dim\": 3072,\r\n",
      "  \"id2label\": {\r\n",
      "    \"0\": \"LABEL_0\",\r\n",
      "    \"1\": \"LABEL_1\",\r\n",
      "    \"2\": \"LABEL_2\",\r\n",
      "    \"3\": \"LABEL_3\",\r\n",
      "    \"4\": \"LABEL_4\"\r\n",
      "  },\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"is_decoder\": false,\r\n",
      "  \"is_encoder_decoder\": false,\r\n",
      "  \"label2id\": {\r\n",
      "    \"LABEL_0\": 0,\r\n",
      "    \"LABEL_1\": 1,\r\n",
      "    \"LABEL_2\": 2,\r\n",
      "    \"LABEL_3\": 3,\r\n",
      "    \"LABEL_4\": 4\r\n",
      "  },\r\n",
      "  \"length_penalty\": 1.0,\r\n",
      "  \"max_length\": 20,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"min_length\": 0,\r\n",
      "  \"model_type\": \"distilbert\",\r\n",
      "  \"n_heads\": 12,\r\n",
      "  \"n_layers\": 6,\r\n",
      "  \"no_repeat_ngram_size\": 0,\r\n",
      "  \"num_beams\": 1,\r\n",
      "  \"num_return_sequences\": 1,\r\n",
      "  \"output_attentions\": false,\r\n",
      "  \"output_hidden_states\": false,\r\n",
      "  \"output_past\": true,\r\n",
      "  \"pad_token_id\": 0,\r\n",
      "  \"prefix\": null,\r\n",
      "  \"pruned_heads\": {},\r\n",
      "  \"qa_dropout\": 0.1,\r\n",
      "  \"repetition_penalty\": 1.0,\r\n",
      "  \"seq_classif_dropout\": 0.2,\r\n",
      "  \"sinusoidal_pos_embds\": false,\r\n",
      "  \"task_specific_params\": null,\r\n",
      "  \"temperature\": 1.0,\r\n",
      "  \"tie_weights_\": true,\r\n",
      "  \"top_k\": 50,\r\n",
      "  \"top_p\": 1.0,\r\n",
      "  \"torchscript\": false,\r\n",
      "  \"use_bfloat16\": false,\r\n",
      "  \"vocab_size\": 30522\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "cat $model_dir/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "loaded_model = TFDistilBertForSequenceClassification.from_pretrained(model_dir,\n",
    "                                                                     id2label={\n",
    "                                                                       0: 1,\n",
    "                                                                       1: 2,\n",
    "                                                                       2: 3,\n",
    "                                                                       3: 4,\n",
    "                                                                       4: 5\n",
    "                                                                     },\n",
    "                                                                     label2id={\n",
    "                                                                       1: 0,\n",
    "                                                                       2: 1,\n",
    "                                                                       3: 2,\n",
    "                                                                       4: 3,\n",
    "                                                                       5: 4\n",
    "                                                                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff35c229bd84700831d546fe5c4cfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "inference_device -1\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "if NUM_GPUS >= 1:\n",
    "    inference_device = 0 # GPU 0\n",
    "else:\n",
    "    inference_device = -1 # CPU\n",
    "print('inference_device {}'.format(inference_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I loved it!  I will recommend this to everyone. [{'label': 5, 'score': 0.786064}]\n",
      "Really bad.  I hope they don't make this anymore. [{'label': 1, 'score': 0.7445503}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "inference_pipeline = TextClassificationPipeline(model=loaded_model, \n",
    "                                                tokenizer=tokenizer,\n",
    "                                                framework='tf',\n",
    "                                                device=inference_device) # -1 is CPU, >= 0 is GPU\n",
    "\n",
    "print(\"\"\"I loved it!  I will recommend this to everyone.\"\"\", inference_pipeline(\"\"\"I loved it!  I will recommend this to everyone.\"\"\"))\n",
    "print(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\", inference_pipeline(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncomment to Start Tensorboard\n",
    "Let's start Tensorboard and point to the tensorboard logs that we downloaded from S3 directly.\n",
    "\n",
    "Note:  If you pointed Tensorboard to S3 directly, you must prepend this command with `S3_REGION=[your-region]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#pip uninstall -y tensorboard-plugin-wit\n",
    "#tensorboard --port 6006 --logdir ./tensorboard/ # <== MAKE SURE YOU INCLUDE THE TRAILING `/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Tensorboard is running locally on your SageMaker Notebook instance, it is reading the training logs from Amazon S3.\n",
    "\n",
    "**MAKE SURE YOU CHANGE `[your-region]` BELOW TO YOUR REGION.**\n",
    "\n",
    "Navigate to https://workshop.notebook.[your-region].sagemaker.aws/proxy/6006/  <== MAKE SURE YOU INCLUDE THE TRAILING SLASH\n",
    "\n",
    "**MAKE SURE YOU CHANGE `[your-region]` ABOVE TO YOUR REGION.**\n",
    "\n",
    "_Note:  Make sure you copy the trailing `/` in the link above.  If you see no data, you are likely not using the correct S3 bucket above._\n",
    "\n",
    "![Tensorboard](img/tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Tensorboard\n",
    "Once you are done, hit `Kernel => Stop` to stop the running `Tensorboard` process in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
