{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Pre-Training, Feature Engineering, and Fine-Tuning with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the entire BERT model needs to be **pre-trained** from scratch because the domain-specific dataset contains words and entities that were not in the original Wikipedia and Google Books training datasets used to create the BERT pre-trained models.  In these cases, the BERT neural network architecture is re-used, but the pre-trained model weights are thrown out and the BERT model is trained from scratch.  Fortunately, BERT has been pre-trained on a very large amount of data and therefore contains a huge vocabulary applicable to a large number of domains including ours.  We will safely re-use and build upon the existing pre-trained BERT model.\n",
    "\n",
    "**Fine-tuning** re-uses the language understanding and semantics learned by the base BERT model (pre-trained on Wikipedia and Google Books) and learns our domain-specific dataset.  Fine-tuning happens very quickly and requires a relatively small number of samples (ie. reviews, in our case).  This translates to lower processing power and lower cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BERT Training](img/bert_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we've already performed the Feature Engineering to create BERT embeddings from the `reviews_body` text using the pre-trained BERT model, and split the dataset into train, validation and test files. To optimize for Tensorflow training, we saved the files in TFRecord format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BERT Pre-Processing](img/bert_preprocessing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s fine-tune the BERT model to our Customer Reviews Dataset and add a new classification layer to predict the `star_rating` for a given `review_body`.\n",
    "\n",
    "As mentioned earlier, BERT’s attention mechanism is called a Transformer. This is, not coincidentally, the name of the popular BERT Python library, “Transformers,” maintained by a company called HuggingFace. We will use a variant of BERT called [DistilBert](https://arxiv.org/pdf/1910.01108.pdf) which requires less memory and compute, but maintains very good accuracy on our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q smdebug==0.7.2\n",
    "!pip install -q sagemaker-experiments==0.1.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the Dataset in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1 - Upload the Features from `./data-tfrecord` to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 cp --recursive ./data-tfrecord/ s3://$bucket/data-tfrecord/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scikit_processing_job_name = 'data-tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store scikit_processing_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2 - Use the Features in S3 from the Previous Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r scikit_processing_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-scikit-learn-2020-04-25-18-17-26-390\n"
     ]
    }
   ],
   "source": [
    "print(scikit_processing_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Scikit Processing Job Name: sagemaker-scikit-learn-2020-04-25-18-17-26-390\n"
     ]
    }
   ],
   "source": [
    "# scikit_processing_job_s3_output_prefix = 'data'\n",
    "print('Previous Scikit Processing Job Name: {}'.format(scikit_processing_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_train = '{}/output/bert-train'.format(scikit_processing_job_name)\n",
    "prefix_validation = '{}/output/bert-validation'.format(scikit_processing_job_name)\n",
    "prefix_test = '{}/output/bert-test'.format(scikit_processing_job_name)\n",
    "\n",
    "train_s3_uri = 's3://{}/{}'.format(bucket, prefix_train)\n",
    "validation_s3_uri = 's3://{}/{}'.format(bucket, prefix_validation)\n",
    "test_s3_uri = 's3://{}/{}'.format(bucket, prefix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-086401037028/sagemaker-scikit-learn-2020-04-25-18-17-26-390/output/bert-train\n",
      "2020-04-25 18:23:34   50961824 part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2020-04-25 18:24:24   71731312 part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "print(train_s3_uri)\n",
    "!aws s3 ls $train_s3_uri/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify S3 Distribution Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-west-2-086401037028/sagemaker-scikit-learn-2020-04-25-18-17-26-390/output/bert-train', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-west-2-086401037028/sagemaker-scikit-learn-2020-04-25-18-17-26-390/output/bert-validation', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-west-2-086401037028/sagemaker-scikit-learn-2020-04-25-18-17-26-390/output/bert-test', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n"
     ]
    }
   ],
   "source": [
    "s3_input_train_data = sagemaker.s3_input(s3_data=train_s3_uri, distribution='ShardedByS3Key') \n",
    "s3_input_validation_data = sagemaker.s3_input(s3_data=validation_s3_uri, distribution='ShardedByS3Key')\n",
    "s3_input_test_data = sagemaker.s3_input(s3_data=test_s3_uri, distribution='ShardedByS3Key')\n",
    "\n",
    "print(s3_input_train_data.config)\n",
    "print(s3_input_validation_data.config)\n",
    "print(s3_input_test_data.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show TensorFlow Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import time\r\n",
      "import random\r\n",
      "import pandas as pd\r\n",
      "from glob import glob\r\n",
      "import pprint\r\n",
      "import argparse\r\n",
      "import json\r\n",
      "import subprocess\r\n",
      "import sys\r\n",
      "import os\r\n",
      "import tensorflow as tf\r\n",
      "#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker-tensorflow==2.1.0.1.0.0'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'smdebug==0.7.2'])\r\n",
      "from transformers import DistilBertTokenizer\r\n",
      "from transformers import TFDistilBertForSequenceClassification\r\n",
      "from transformers import TextClassificationPipeline\r\n",
      "from transformers.configuration_distilbert import DistilBertConfig\r\n",
      "\r\n",
      "CLASSES = [1, 2, 3, 4, 5]\r\n",
      "\r\n",
      "def select_data_and_label_from_record(record):\r\n",
      "    x = {\r\n",
      "        'input_ids': record['input_ids'],\r\n",
      "        'input_mask': record['input_mask'],\r\n",
      "        'segment_ids': record['segment_ids']\r\n",
      "    }\r\n",
      "\r\n",
      "    y = record['label_ids']\r\n",
      "\r\n",
      "    return (x, y)\r\n",
      "\r\n",
      "\r\n",
      "def file_based_input_dataset_builder(channel,\r\n",
      "                                     input_filenames,\r\n",
      "                                     pipe_mode,\r\n",
      "                                     is_training,\r\n",
      "                                     drop_remainder,\r\n",
      "                                     batch_size,\r\n",
      "                                     epochs,\r\n",
      "                                     steps_per_epoch,\r\n",
      "                                     max_seq_length):\r\n",
      "\r\n",
      "    # For training, we want a lot of parallel reading and shuffling.\r\n",
      "    # For eval, we want no shuffling and parallel reading doesn't matter.\r\n",
      "\r\n",
      "    if pipe_mode:\r\n",
      "        print('***** Using pipe_mode with channel {}'.format(channel))\r\n",
      "        from sagemaker_tensorflow import PipeModeDataset\r\n",
      "        dataset = PipeModeDataset(channel=channel,\r\n",
      "                                  record_format='TFRecord')\r\n",
      "    else:\r\n",
      "        print('***** Using input_filenames {}'.format(input_filenames))\r\n",
      "        dataset = tf.data.TFRecordDataset(input_filenames)\r\n",
      "\r\n",
      "    dataset = dataset.repeat(epochs * steps_per_epoch)\r\n",
      "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
      "\r\n",
      "    name_to_features = {\r\n",
      "      \"input_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \"input_mask\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \"segment_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\r\n",
      "    }\r\n",
      "\r\n",
      "    def _decode_record(record, name_to_features):\r\n",
      "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\r\n",
      "        record = tf.io.parse_single_example(record, name_to_features)\r\n",
      "        # TODO:  wip/bert/bert_attention_head_view/train.py\r\n",
      "        # Convert input_ids into input_tokens with DistilBert vocabulary \r\n",
      "        #  if hook.get_collections()['all'].save_config.should_save_step(modes.EVAL, hook.mode_steps[modes.EVAL]):\r\n",
      "#              hook._write_raw_tensor_simple(\"input_tokens\", input_tokens)\r\n",
      "        return record\r\n",
      "    \r\n",
      "    dataset = dataset.apply(\r\n",
      "        tf.data.experimental.map_and_batch(\r\n",
      "          lambda record: _decode_record(record, name_to_features),\r\n",
      "          batch_size=batch_size,\r\n",
      "          drop_remainder=drop_remainder,\r\n",
      "          num_parallel_calls=tf.data.experimental.AUTOTUNE))\r\n",
      "\r\n",
      "    dataset.cache()\r\n",
      "\r\n",
      "    if is_training:\r\n",
      "        dataset = dataset.shuffle(seed=42,\r\n",
      "                                  buffer_size=1000,\r\n",
      "                                  reshuffle_each_iteration=True)\r\n",
      "\r\n",
      "    return dataset\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    parser.add_argument('--train_data', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_CHANNEL_TRAIN'])\r\n",
      "    parser.add_argument('--validation_data', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_CHANNEL_VALIDATION'])\r\n",
      "    parser.add_argument('--test_data',\r\n",
      "                        type=str,\r\n",
      "                        default=os.environ['SM_CHANNEL_TEST'])\r\n",
      "    # This points to the S3 location - this should not be used by our code\r\n",
      "    # We should use /opt/ml/model/ instead\r\n",
      "#     parser.add_argument('--model_dir', \r\n",
      "#                         type=str, \r\n",
      "#                         default=os.environ['SM_MODEL_DIR'])\r\n",
      "    parser.add_argument('--output_dir',\r\n",
      "                        type=str,\r\n",
      "                        default=os.environ['SM_OUTPUT_DIR'])\r\n",
      "    # This is unused\r\n",
      "    parser.add_argument('--output_data_dir',\r\n",
      "                        type=str,\r\n",
      "                        default=os.environ['SM_OUTPUT_DATA_DIR'])\r\n",
      "    parser.add_argument('--hosts', \r\n",
      "                        type=list, \r\n",
      "                        default=json.loads(os.environ['SM_HOSTS']))\r\n",
      "    parser.add_argument('--current_host', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_CURRENT_HOST'])    \r\n",
      "    parser.add_argument('--num_gpus', \r\n",
      "                        type=int, \r\n",
      "                        default=os.environ['SM_NUM_GPUS'])\r\n",
      "    parser.add_argument('--use_xla',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--use_amp',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--max_seq_length',\r\n",
      "                        type=int,\r\n",
      "                        default=128)\r\n",
      "    parser.add_argument('--train_batch_size',\r\n",
      "                        type=int,\r\n",
      "                        default=128)\r\n",
      "    parser.add_argument('--validation_batch_size',\r\n",
      "                        type=int,\r\n",
      "                        default=256)\r\n",
      "    parser.add_argument('--test_batch_size',\r\n",
      "                        type=int,\r\n",
      "                        default=256)\r\n",
      "    parser.add_argument('--epochs',\r\n",
      "                        type=int,\r\n",
      "                        default=2)\r\n",
      "    parser.add_argument('--learning_rate',\r\n",
      "                        type=float,\r\n",
      "                        default=0.00003)\r\n",
      "    parser.add_argument('--epsilon',\r\n",
      "                        type=float,\r\n",
      "                        default=0.00000001)\r\n",
      "    parser.add_argument('--train_steps_per_epoch',\r\n",
      "                        type=int,\r\n",
      "                        default=1000)\r\n",
      "    parser.add_argument('--validation_steps',\r\n",
      "                        type=int,\r\n",
      "                        default=1000)\r\n",
      "    parser.add_argument('--test_steps',\r\n",
      "                        type=int,\r\n",
      "                        default=1000)\r\n",
      "    parser.add_argument('--freeze_bert_layer',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--enable_sagemaker_debugger',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--run_validation',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)    \r\n",
      "    parser.add_argument('--run_test',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)    \r\n",
      "    parser.add_argument('--run_sample_predictions',\r\n",
      "                        type=eval,\r\n",
      "                        default=False)\r\n",
      "    \r\n",
      "    args, _ = parser.parse_known_args()\r\n",
      "    print(\"Args:\") \r\n",
      "    print(args)\r\n",
      "    \r\n",
      "    env_var = os.environ \r\n",
      "    print(\"Environment Variables:\") \r\n",
      "    pprint.pprint(dict(env_var), width = 1) \r\n",
      "\r\n",
      "    train_data = args.train_data\r\n",
      "    print('train_data {}'.format(train_data))\r\n",
      "    validation_data = args.validation_data\r\n",
      "    print('validation_data {}'.format(validation_data))\r\n",
      "    test_data = args.test_data\r\n",
      "    print('test_data {}'.format(test_data))    \r\n",
      "\r\n",
      "#    model_dir = args.model_dir\r\n",
      "#    print('model_dir {}'.format(model_dir))    \r\n",
      "    local_model_dir = os.environ['SM_MODEL_DIR']\r\n",
      "\r\n",
      "    output_dir = args.output_dir\r\n",
      "    print('output_dir {}'.format(output_dir))    \r\n",
      "\r\n",
      "    # This is unused\r\n",
      "#    output_data_dir = args.output_data_dir\r\n",
      "#    print('output_data_dir {}'.format(output_data_dir))    \r\n",
      "    hosts = args.hosts\r\n",
      "    print('hosts {}'.format(hosts))    \r\n",
      "    current_host = args.current_host\r\n",
      "    print('current_host {}'.format(current_host))    \r\n",
      "    num_gpus = args.num_gpus\r\n",
      "    print('num_gpus {}'.format(num_gpus))\r\n",
      "    use_xla = args.use_xla\r\n",
      "    print('use_xla {}'.format(use_xla))    \r\n",
      "    use_amp = args.use_amp\r\n",
      "    print('use_amp {}'.format(use_amp))    \r\n",
      "    max_seq_length = args.max_seq_length\r\n",
      "    print('max_seq_length {}'.format(max_seq_length))    \r\n",
      "    train_batch_size = args.train_batch_size\r\n",
      "    print('train_batch_size {}'.format(train_batch_size))    \r\n",
      "    validation_batch_size = args.validation_batch_size\r\n",
      "    print('validation_batch_size {}'.format(validation_batch_size))    \r\n",
      "    test_batch_size = args.test_batch_size\r\n",
      "    print('test_batch_size {}'.format(test_batch_size))    \r\n",
      "    epochs = args.epochs\r\n",
      "    print('epochs {}'.format(epochs))    \r\n",
      "    learning_rate = args.learning_rate\r\n",
      "    print('learning_rate {}'.format(learning_rate))    \r\n",
      "    epsilon = args.epsilon\r\n",
      "    print('epsilon {}'.format(epsilon))    \r\n",
      "    train_steps_per_epoch = args.train_steps_per_epoch\r\n",
      "    print('train_steps_per_epoch {}'.format(train_steps_per_epoch))    \r\n",
      "    validation_steps = args.validation_steps\r\n",
      "    print('validation_steps {}'.format(validation_steps))    \r\n",
      "    test_steps = args.test_steps\r\n",
      "    print('test_steps {}'.format(test_steps))    \r\n",
      "    freeze_bert_layer = args.freeze_bert_layer\r\n",
      "    print('freeze_bert_layer {}'.format(freeze_bert_layer))    \r\n",
      "    enable_sagemaker_debugger = args.enable_sagemaker_debugger\r\n",
      "    print('enable_sagemaker_debugger {}'.format(enable_sagemaker_debugger))    \r\n",
      "    run_validation = args.run_validation\r\n",
      "    print('run_validation {}'.format(run_validation))    \r\n",
      "    run_test = args.run_test\r\n",
      "    print('run_test {}'.format(run_test))    \r\n",
      "    run_sample_predictions = args.run_sample_predictions\r\n",
      "    print('run_sample_predictions {}'.format(run_sample_predictions))    \r\n",
      "\r\n",
      "    # Determine if PipeMode is enabled \r\n",
      "    pipe_mode_str = os.environ.get('SM_INPUT_DATA_CONFIG', '')\r\n",
      "    pipe_mode = (pipe_mode_str.find('Pipe') >= 0)\r\n",
      "    print('Using pipe_mode: {}'.format(pipe_mode))\r\n",
      " \r\n",
      "    # Model Output \r\n",
      "    transformer_fine_tuned_model_path = os.path.join(local_model_dir, 'transformers/fine-tuned/')\r\n",
      "    os.makedirs(transformer_fine_tuned_model_path, exist_ok=True)\r\n",
      "\r\n",
      "    # SavedModel Output\r\n",
      "    tensorflow_saved_model_path = os.path.join(local_model_dir, 'tensorflow/saved_model/0')\r\n",
      "    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\r\n",
      "\r\n",
      "    # Tensorboard Logs \r\n",
      "    tensorboard_logs_path = os.path.join(output_dir, 'tensorboard') \r\n",
      "    os.makedirs(tensorboard_logs_path, exist_ok=True)\r\n",
      "\r\n",
      "    distributed_strategy = tf.distribute.MirroredStrategy()\r\n",
      "    # smdebug currently (0.7.2) does not support MultiWorkerMirroredStrategy()\r\n",
      "    # distributed_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n",
      "    with distributed_strategy.scope():\r\n",
      "        tf.config.optimizer.set_jit(use_xla)\r\n",
      "        tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": use_amp})\r\n",
      "\r\n",
      "        train_data_filenames = glob(os.path.join(train_data, '*.tfrecord'))\r\n",
      "        print('train_data_filenames {}'.format(train_data_filenames))\r\n",
      "        train_dataset = file_based_input_dataset_builder(\r\n",
      "            channel='train',\r\n",
      "            input_filenames=train_data_filenames,\r\n",
      "            pipe_mode=pipe_mode,\r\n",
      "            is_training=True,\r\n",
      "            drop_remainder=False,\r\n",
      "            batch_size=train_batch_size,\r\n",
      "            epochs=epochs,\r\n",
      "            steps_per_epoch=train_steps_per_epoch,\r\n",
      "            max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "\r\n",
      "        tokenizer = None\r\n",
      "        config = None\r\n",
      "        model = None\r\n",
      "\r\n",
      "        # This is required when launching many instances at once...  the urllib request seems to get denied periodically\r\n",
      "        successful_download = False\r\n",
      "        retries = 0\r\n",
      "        while (retries < 5 and not successful_download):\r\n",
      "            try:\r\n",
      "                tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\r\n",
      "                config = DistilBertConfig.from_pretrained('distilbert-base-uncased',\r\n",
      "                                                          num_labels=len(CLASSES))\r\n",
      "                model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\r\n",
      "                                                                              config=config)\r\n",
      "                successful_download = True\r\n",
      "                print('Sucessfully downloaded after {} retries.'.format(retries))\r\n",
      "            except:\r\n",
      "                retries = retries + 1\r\n",
      "                random_sleep = random.randint(1, 30)\r\n",
      "                print('Retry #{}.  Sleeping for {} seconds'.format(retries, random_sleep))\r\n",
      "                time.sleep(random_sleep)\r\n",
      "\r\n",
      "        if not tokenizer or not model or not config:\r\n",
      "            print('Not properly initialized...')\r\n",
      "\r\n",
      "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\r\n",
      "        print('** use_amp {}'.format(use_amp))        \r\n",
      "        if use_amp:\r\n",
      "            # loss scaling is currently required when using mixed precision\r\n",
      "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, 'dynamic')\r\n",
      "\r\n",
      "        callbacks = []\r\n",
      "        print('enable_sagemaker_debugger {}'.format(enable_sagemaker_debugger))\r\n",
      "        if enable_sagemaker_debugger:\r\n",
      "            print('*** DEBUGGING ***')\r\n",
      "            import smdebug.tensorflow as smd\r\n",
      "            # This assumes that we specified debugger_hook_config\r\n",
      "            callback = smd.KerasHook.create_from_json_file()\r\n",
      "            print(callback)\r\n",
      "            print('*** CALLBACK {} ***'.format(callback))\r\n",
      "            callbacks.append(callback)\r\n",
      "            optimizer = callback.wrap_optimizer(optimizer)\r\n",
      "\r\n",
      "        callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_logs_path)\r\n",
      "        callbacks.append(callback)\r\n",
      "            \r\n",
      "        print('*** OPTIMIZER {} ***'.format(optimizer))\r\n",
      "        \r\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
      "        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\r\n",
      "\r\n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\r\n",
      "        print('Trained model {}'.format(model))\r\n",
      "        print(model.summary())\r\n",
      "\r\n",
      "        if run_validation:\r\n",
      "            validation_data_filenames = glob(os.path.join(validation_data, '*.tfrecord'))\r\n",
      "            print('validation_data_filenames {}'.format(validation_data_filenames))\r\n",
      "            validation_dataset = file_based_input_dataset_builder(\r\n",
      "                channel='validation',\r\n",
      "                input_filenames=validation_data_filenames,\r\n",
      "                pipe_mode=pipe_mode,\r\n",
      "                is_training=False,\r\n",
      "                drop_remainder=False,\r\n",
      "                batch_size=validation_batch_size,\r\n",
      "                epochs=epochs,\r\n",
      "                steps_per_epoch=validation_steps,\r\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "            \r\n",
      "            # HACK:  trim the Validation dataset down to equal the number of validation steps to workaround PipeMode issue\r\n",
      "            validation_dataset = validation_dataset.take(validation_steps)\r\n",
      "            \r\n",
      "            train_and_validation_history = model.fit(train_dataset,\r\n",
      "                                                     shuffle=True,\r\n",
      "                                                     epochs=epochs,\r\n",
      "                                                     steps_per_epoch=train_steps_per_epoch,\r\n",
      "                                                     validation_data=validation_dataset,\r\n",
      "                                                     validation_steps=validation_steps,\r\n",
      "                                                     callbacks=callbacks)\r\n",
      "            print(train_and_validation_history)\r\n",
      "        else: # Not running validation\r\n",
      "            train_history = model.fit(train_dataset,\r\n",
      "                                      shuffle=True,\r\n",
      "                                      epochs=epochs,\r\n",
      "                                      steps_per_epoch=train_steps_per_epoch,\r\n",
      "                                      callbacks=callbacks)\r\n",
      "            print(train_history)\r\n",
      "\r\n",
      "        if run_test:\r\n",
      "            test_data_filenames = glob(os.path.join(test_data, '*.tfrecord'))\r\n",
      "            print('test_data_filenames {}'.format(test_data_filenames))\r\n",
      "            test_dataset = file_based_input_dataset_builder(\r\n",
      "                channel='test',\r\n",
      "                input_filenames=test_data_filenames,\r\n",
      "                pipe_mode=pipe_mode,\r\n",
      "                is_training=False,\r\n",
      "                drop_remainder=False,\r\n",
      "                batch_size=test_batch_size,\r\n",
      "                epochs=epochs,\r\n",
      "                steps_per_epoch=test_steps,\r\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "\r\n",
      "            test_history = model.evaluate(test_dataset,\r\n",
      "                                          steps=test_steps,\r\n",
      "                                          callbacks=callbacks)\r\n",
      "            print(test_history)\r\n",
      "\r\n",
      "            \r\n",
      "        # Save the fine-tuned Transformers Model\r\n",
      "        print('transformer_fine_tuned_model_path {}'.format(transformer_fine_tuned_model_path))   \r\n",
      "\r\n",
      "        model.save_pretrained(transformer_fine_tuned_model_path)\r\n",
      "\r\n",
      "        # Save the TensorFlow SavedModel\r\n",
      "        print('tensorflow_saved_model_path {}'.format(tensorflow_saved_model_path))   \r\n",
      "        model.save(tensorflow_saved_model_path, save_format='tf')\r\n",
      "\r\n",
      "    if run_sample_predictions:\r\n",
      "        loaded_model = TFDistilBertForSequenceClassification.from_pretrained(transformer_fine_tuned_model_path,\r\n",
      "                                                                       id2label={\r\n",
      "                                                                        0: 1,\r\n",
      "                                                                        1: 2,\r\n",
      "                                                                        2: 3,\r\n",
      "                                                                        3: 4,\r\n",
      "                                                                        4: 5\r\n",
      "                                                                       },\r\n",
      "                                                                       label2id={\r\n",
      "                                                                        1: 0,\r\n",
      "                                                                        2: 1,\r\n",
      "                                                                        3: 2,\r\n",
      "                                                                        4: 3,\r\n",
      "                                                                        5: 4\r\n",
      "                                                                       })\r\n",
      "\r\n",
      "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\r\n",
      "\r\n",
      "        if num_gpus >= 1:\r\n",
      "            inference_device = 0 # GPU 0\r\n",
      "        else:\r\n",
      "            inference_device = -1 # CPU\r\n",
      "        print('inference_device {}'.format(inference_device))\r\n",
      "\r\n",
      "        inference_pipeline = TextClassificationPipeline(model=loaded_model, \r\n",
      "                                                        tokenizer=tokenizer,\r\n",
      "                                                        framework='tf',\r\n",
      "                                                        device=inference_device)  \r\n",
      "\r\n",
      "        print(\"\"\"I loved it!  I will recommend this to everyone.\"\"\", inference_pipeline(\"\"\"I loved it!  I will recommend this to everyone.\"\"\"))\r\n",
      "        print(\"\"\"It's OK.\"\"\", inference_pipeline(\"\"\"It's OK.\"\"\"))\r\n",
      "        print(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\", inference_pipeline(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\"))\r\n"
     ]
    }
   ],
   "source": [
    "!cat src/tf_bert_reviews.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hyper-Parameters For Classification Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1\n",
    "learning_rate=0.00001\n",
    "epsilon=0.00000001\n",
    "train_batch_size=128\n",
    "validation_batch_size=128\n",
    "test_batch_size=128\n",
    "train_steps_per_epoch=1000\n",
    "validation_steps=1000\n",
    "test_steps=1000\n",
    "train_instance_count=1\n",
    "train_instance_type='ml.p3.2xlarge'\n",
    "train_volume_size=1024\n",
    "use_xla=True\n",
    "use_amp=True\n",
    "max_seq_length=128                                # must match setting used when engineering features \n",
    "freeze_bert_layer=False\n",
    "enable_sagemaker_debugger=True                    # Enable SM Debugger\n",
    "input_mode='Pipe'                                 # 'File' or 'Pipe' Mode == similar to Linux pipes, loaded as needed\n",
    "run_validation=True\n",
    "run_test=True\n",
    "run_sample_predictions=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup SageMaker Experiment Tracking For Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = '{}'.format(int(time.time()))\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "experiment=Experiment.create(\n",
    "    experiment_name='Train-Reviews-BERT-Experiment-{}'.format(timestamp),\n",
    "    description='Train Reviews BERT', \n",
    "    sagemaker_boto_client=sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log The Training Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Reviews-BERT-Experiment-1587839693\n"
     ]
    }
   ],
   "source": [
    "from smexperiments.tracker import Tracker\n",
    "tracker_display_name='Train-Reviews-BERT-Experiment-{}'.format(timestamp)\n",
    "print(tracker_display_name)\n",
    "\n",
    "tracker = Tracker.create(display_name=tracker_display_name, sagemaker_boto_client=sm)\n",
    "tracker.log_parameters({\n",
    "    'epochs': epochs,\n",
    "    'learning_rate': learning_rate,\n",
    "    'epsilon': epsilon,\n",
    "    'train_batch_size': train_batch_size,\n",
    "    'validation_batch_size': validation_batch_size,\n",
    "    'test_batch_size': test_batch_size,\n",
    "    'train_steps_per_epoch': train_steps_per_epoch,\n",
    "    'validation_steps': validation_steps,\n",
    "    'test_steps': test_steps,\n",
    "    'train_instance_count': train_instance_count,\n",
    "    'train_instance_type': train_instance_type,\n",
    "    'train_volume_size': train_volume_size,\n",
    "    'use_xla': use_xla,\n",
    "    'use_amp': use_amp,\n",
    "    'max_seq_length': max_seq_length,\n",
    "    'freeze_bert_layer': freeze_bert_layer,\n",
    "    'enable_sagemaker_debugger': enable_sagemaker_debugger,\n",
    "    'input_mode': input_mode, \n",
    "    'run_validation': run_validation,\n",
    "    'run_test': run_test,\n",
    "    'run_sample_predictions': run_sample_predictions,    \n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log the S3 Input Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.log_input(name='reviews_dataset_train', media_type='s3/uri', value=train_s3_uri)\n",
    "tracker.log_input(name='reviews_dataset_validation', media_type='s3/uri', value=validation_s3_uri)\n",
    "tracker.log_input(name='reviews_dataset_test', media_type='s3/uri', value=test_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify a Trial for this Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.trial import Trial\n",
    "trial_name='Train-Reviews-BERT-Trial-{}'.format(timestamp)\n",
    "trial = Trial.create(trial_name=trial_name, \n",
    "                     experiment_name=experiment.experiment_name, \n",
    "                     sagemaker_boto_client=sm)\n",
    "trial.add_trial_component(tracker.trial_component)\n",
    "trial_component_display_name='Train-Reviews-BERT-Trial-{}'.format(timestamp)\n",
    "experiment_config={'ExperimentName': experiment.experiment_name,\n",
    "                   'TrialName': trial.trial_name,\n",
    "                   'TrialComponentDisplayName': trial_component_display_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Metrics To Track Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_definitions = [\n",
    "     {'Name': 'train:loss', 'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'train:accuracy', 'Regex': 'accuracy: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:loss', 'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:accuracy', 'Regex': 'val_accuracy: ([0-9\\\\.]+)'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup SageMaker Debugger\n",
    "Define Debugger Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule\n",
    "from sagemaker.debugger import rule_configs\n",
    "from sagemaker.debugger import CollectionConfig\n",
    "from sagemaker.debugger import DebuggerHookConfig\n",
    "\n",
    "rules=[\n",
    "        Rule.sagemaker(\n",
    "            rule_configs.loss_not_decreasing(),\n",
    "            rule_parameters={\n",
    "                'collection_names': 'losses,metrics',\n",
    "                'use_losses_collection': 'true',\n",
    "                'num_steps': '10',\n",
    "                'diff_percent': '50'\n",
    "            },\n",
    "            collections_to_save=[\n",
    "                CollectionConfig(name='losses',\n",
    "                                 parameters={\n",
    "                                     'save_interval': '10',\n",
    "                                 }),\n",
    "                CollectionConfig(name='metrics',\n",
    "                                 parameters={\n",
    "                                     'save_interval': '10',\n",
    "                                 })\n",
    "            ]\n",
    "        ),\n",
    "        Rule.sagemaker(\n",
    "            rule_configs.overtraining(),\n",
    "            rule_parameters={\n",
    "                'collection_names': 'losses,metrics',\n",
    "                'patience_train': '10',\n",
    "                'patience_validation': '10',\n",
    "                'delta': '0.5'\n",
    "            },\n",
    "            collections_to_save=[\n",
    "                CollectionConfig(name='losses',\n",
    "                                 parameters={\n",
    "                                     'save_interval': '10',\n",
    "                                 }),\n",
    "                CollectionConfig(name='metrics',\n",
    "                                 parameters={\n",
    "                                     'save_interval': '10',\n",
    "                                 })\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "hook_config = DebuggerHookConfig(\n",
    "    hook_parameters={\n",
    "        'save_interval': '10', # number of steps\n",
    "        'export_tensorboard': 'true',\n",
    "        'tensorboard_dir': 'hook_tensorboard/',\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Our BERT + TensorFlow Script to Run on SageMaker\n",
    "Prepare our TensorFlow model to run on the managed SageMaker service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(entry_point='tf_bert_reviews.py', \n",
    "                       source_dir='src', # put requirements.txt in this directory and it gets picked up\n",
    "                       role=role,\n",
    "                       train_instance_count=train_instance_count, # Make sure you have at least this number of input files or the ShardedByS3Key distibution strategy will fail the job due to no data available\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_volume_size=train_volume_size,\n",
    "                       py_version='py3',\n",
    "                       framework_version='2.1.0',\n",
    "                       hyperparameters={'epochs': epochs,\n",
    "                                        'learning_rate': learning_rate,\n",
    "                                        'epsilon': epsilon,\n",
    "                                        'train_batch_size': train_batch_size,\n",
    "                                        'validation_batch_size': validation_batch_size,\n",
    "                                        'test_batch_size': test_batch_size,                                             \n",
    "                                        'train_steps_per_epoch': train_steps_per_epoch,\n",
    "                                        'validation_steps': validation_steps,\n",
    "                                        'test_steps': test_steps,\n",
    "                                        'use_xla': use_xla,\n",
    "                                        'use_amp': use_amp,                                             \n",
    "                                        'max_seq_length': max_seq_length,\n",
    "                                        'freeze_bert_layer': freeze_bert_layer,\n",
    "                                        'enable_sagemaker_debugger': enable_sagemaker_debugger,\n",
    "                                        'run_validation': run_validation,\n",
    "                                        'run_test': run_test,\n",
    "                                        'run_sample_predictions': run_sample_predictions},\n",
    "                       input_mode=input_mode,\n",
    "                       metric_definitions=metrics_definitions,\n",
    "                       rules=rules,\n",
    "                       debugger_hook_config=hook_config,                       \n",
    "                       train_max_run=7200 # max 2 hours * 60 minutes seconds per hour * 60 seconds per minute\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: tensorflow-training-2020-04-25-18-34-53-740\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs={'train': s3_input_train_data, \n",
    "                      'validation': s3_input_validation_data,\n",
    "                      'test': s3_input_test_data\n",
    "                     },\n",
    "                     experiment_config=experiment_config,                   \n",
    "                     wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Job Name:  tensorflow-training-2020-04-25-18-34-53-740\n"
     ]
    }
   ],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "print('Training Job Name:  {}'.format(training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region=us-west-2#/jobs/tensorflow-training-2020-04-25-18-34-53-740\">Training Job</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/jobs/{}\">Training Job</a> After About 5 Minutes</b>'.format(region, training_job_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/sagemaker/TrainingJobs;prefix=tensorflow-training-2020-04-25-18-34-53-740;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, training_job_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-086401037028/tensorflow-training-2020-04-25-18-34-53-740/?region=us-west-2&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>'.format(bucket, training_job_name, region)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait Until the Training Job Completes Above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=sess, \n",
    "    experiment_name=experiment.experiment_name,\n",
    "    sort_by='metrics.validation:accuracy.max',  # Sorting By Validation Accuracy\n",
    "    sort_order='Descending',\n",
    "    metric_names=['validation:accuracy'],\n",
    "    parameter_names=['epochs', 'train_batch_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>epochs</th>\n",
       "      <th>train_batch_size</th>\n",
       "      <th>validation:accuracy - Min</th>\n",
       "      <th>validation:accuracy - Max</th>\n",
       "      <th>validation:accuracy - Avg</th>\n",
       "      <th>validation:accuracy - StdDev</th>\n",
       "      <th>validation:accuracy - Last</th>\n",
       "      <th>validation:accuracy - Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensorflow-training-2020-04-25-18-34-53-740-aw...</td>\n",
       "      <td>Train-Reviews-BERT-Trial-1587839693</td>\n",
       "      <td>arn:aws:sagemaker:us-west-2:086401037028:train...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.7198</td>\n",
       "      <td>0.7198</td>\n",
       "      <td>0.7198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7198</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TrialComponent-2020-04-25-183453-qtks</td>\n",
       "      <td>Train-Reviews-BERT-Experiment-1587839693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  TrialComponentName  \\\n",
       "0  tensorflow-training-2020-04-25-18-34-53-740-aw...   \n",
       "1              TrialComponent-2020-04-25-183453-qtks   \n",
       "\n",
       "                                DisplayName  \\\n",
       "0       Train-Reviews-BERT-Trial-1587839693   \n",
       "1  Train-Reviews-BERT-Experiment-1587839693   \n",
       "\n",
       "                                           SourceArn  epochs  \\\n",
       "0  arn:aws:sagemaker:us-west-2:086401037028:train...     1.0   \n",
       "1                                                NaN     NaN   \n",
       "\n",
       "   train_batch_size  validation:accuracy - Min  validation:accuracy - Max  \\\n",
       "0             128.0                     0.7198                     0.7198   \n",
       "1               NaN                        NaN                        NaN   \n",
       "\n",
       "   validation:accuracy - Avg  validation:accuracy - StdDev  \\\n",
       "0                     0.7198                           0.0   \n",
       "1                        NaN                           NaN   \n",
       "\n",
       "   validation:accuracy - Last  validation:accuracy - Count  \n",
       "0                      0.7198                          1.0  \n",
       "1                         NaN                          NaN  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analytics_table = trial_component_analytics.dataframe()\n",
    "analytics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>SageMaker.ImageUri</th>\n",
       "      <th>SageMaker.InstanceCount</th>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <th>SageMaker.VolumeSizeInGB</th>\n",
       "      <th>enable_sagemaker_debugger</th>\n",
       "      <th>epochs</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>...</th>\n",
       "      <th>train:accuracy - Avg</th>\n",
       "      <th>train:accuracy - StdDev</th>\n",
       "      <th>train:accuracy - Last</th>\n",
       "      <th>train:accuracy - Count</th>\n",
       "      <th>loss_EVAL - Min</th>\n",
       "      <th>loss_EVAL - Max</th>\n",
       "      <th>loss_EVAL - Avg</th>\n",
       "      <th>loss_EVAL - StdDev</th>\n",
       "      <th>loss_EVAL - Last</th>\n",
       "      <th>loss_EVAL - Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TrialComponent-2020-04-25-183453-qtks</td>\n",
       "      <td>Train-Reviews-BERT-Experiment-1587839693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensorflow-training-2020-04-25-18-34-53-740-aw...</td>\n",
       "      <td>Train-Reviews-BERT-Trial-1587839693</td>\n",
       "      <td>arn:aws:sagemaker:us-west-2:086401037028:train...</td>\n",
       "      <td>763104351884.dkr.ecr.us-west-2.amazonaws.com/t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ml.p3.2xlarge</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>true</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634206</td>\n",
       "      <td>0.139124</td>\n",
       "      <td>0.7248</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.534383</td>\n",
       "      <td>0.88062</td>\n",
       "      <td>0.710282</td>\n",
       "      <td>0.068981</td>\n",
       "      <td>0.667141</td>\n",
       "      <td>201.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  TrialComponentName  \\\n",
       "0              TrialComponent-2020-04-25-183453-qtks   \n",
       "1  tensorflow-training-2020-04-25-18-34-53-740-aw...   \n",
       "\n",
       "                                DisplayName  \\\n",
       "0  Train-Reviews-BERT-Experiment-1587839693   \n",
       "1       Train-Reviews-BERT-Trial-1587839693   \n",
       "\n",
       "                                           SourceArn  \\\n",
       "0                                                NaN   \n",
       "1  arn:aws:sagemaker:us-west-2:086401037028:train...   \n",
       "\n",
       "                                  SageMaker.ImageUri  SageMaker.InstanceCount  \\\n",
       "0                                                NaN                      NaN   \n",
       "1  763104351884.dkr.ecr.us-west-2.amazonaws.com/t...                      1.0   \n",
       "\n",
       "  SageMaker.InstanceType  SageMaker.VolumeSizeInGB enable_sagemaker_debugger  \\\n",
       "0                    NaN                       NaN                       NaN   \n",
       "1          ml.p3.2xlarge                    1024.0                      true   \n",
       "\n",
       "   epochs       epsilon  ... train:accuracy - Avg  train:accuracy - StdDev  \\\n",
       "0     NaN           NaN  ...                  NaN                      NaN   \n",
       "1     1.0  1.000000e-08  ...             0.634206                 0.139124   \n",
       "\n",
       "   train:accuracy - Last train:accuracy - Count loss_EVAL - Min  \\\n",
       "0                    NaN                    NaN             NaN   \n",
       "1                 0.7248                   17.0        0.534383   \n",
       "\n",
       "  loss_EVAL - Max loss_EVAL - Avg  loss_EVAL - StdDev loss_EVAL - Last  \\\n",
       "0             NaN             NaN                 NaN              NaN   \n",
       "1         0.88062        0.710282            0.068981         0.667141   \n",
       "\n",
       "  loss_EVAL - Count  \n",
       "0               NaN  \n",
       "1             201.0  \n",
       "\n",
       "[2 rows x 115 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineage_table = ExperimentAnalytics(\n",
    "    sagemaker_session=sess,\n",
    "    experiment_name=experiment.experiment_name,\n",
    "    sort_by=\"CreationTime\",\n",
    "    sort_order=\"Ascending\",\n",
    ")\n",
    "lineage_table.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Debugger Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'RuleConfigurationName': 'LossNotDecreasing',\n",
       "  'RuleEvaluationJobArn': 'arn:aws:sagemaker:us-west-2:086401037028:processing-job/tensorflow-training-2020-0-lossnotdecreasing-18545fcf',\n",
       "  'RuleEvaluationStatus': 'InProgress',\n",
       "  'LastModifiedTime': datetime.datetime(2020, 4, 25, 18, 45, 26, 249000, tzinfo=tzlocal())},\n",
       " {'RuleConfigurationName': 'Overtraining',\n",
       "  'RuleEvaluationJobArn': 'arn:aws:sagemaker:us-west-2:086401037028:processing-job/tensorflow-training-2020-0-overtraining-969ea0ef',\n",
       "  'RuleEvaluationStatus': 'IssuesFound',\n",
       "  'StatusDetails': 'RuleEvaluationConditionMet: Evaluation of the rule Overtraining at step 1100 resulted in the condition being met\\n',\n",
       "  'LastModifiedTime': datetime.datetime(2020, 4, 25, 18, 45, 26, 249000, tzinfo=tzlocal())}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.latest_training_job.rule_job_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-086401037028/tensorflow-training-2020-04-25-18-34-53-740/debug-output\n"
     ]
    }
   ],
   "source": [
    "training_job_debugger_artifacts_path = estimator.latest_job_debugger_artifacts_path()\n",
    "print(training_job_debugger_artifacts_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass Variables to the Next Notebook(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'training_job_debugger_artifacts_path' (str)\n"
     ]
    }
   ],
   "source": [
    "%store training_job_debugger_artifacts_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'training_job_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
